{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_d5_m1_image_comparison.ipynb","provenance":[{"file_id":"1BlKp2nueAHvFCBUB-v2o2adTvB3sYJrv","timestamp":1626302866387}],"collapsed_sections":[],"authorship_tag":"ABX9TyPswcZo/bx+irIRW3BM6y2h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zd-FCo-G71ST"},"source":["# Image Matching Overview"]},{"cell_type":"markdown","metadata":{"id":"bdtcNSf83ijL"},"source":["In this module we will experiment with two domain of computational image analysis:  1) Comparison and Classification.  Matching describes the domain of effort where we are primarily concerned with computationally determining the degree to which any two images are the same.  For example, we may have a copy of a woodblock print that have found in one collection and want to know if other copies of the same print exist in other collections. \n","\n","The Classification domain is primarily focused on the degree to which images are similar to each other.  For example, we want to train a computer to search through large image catalogues and automatically separate all photographs of paintings from photographs of digital prints.  The OCR processes with which you have already worked are a good example of a classification problems.  The letter “D” appears in many fonts and sizes, many of which differ significantly, but as humans we still recognize them as belong to category (or class) of “D.”\n","\n","The desire the to deploy computational methods in both domains is driven largely by the problem of scale and the cognitive difficulty that we, as humans, encounter when trying to work with very large image collections—a problem that is amplified in this era of mass digitization.  But here, as elsewhere, we should not overlook the value that computers can bring to our work as intelligent provocateurs.  When we deploy computational methods, we frequently get seemingly bizarre and incorrect results.  However, we should not be so quick dismiss results that we fail to understand.  \n","\n","Computers are, if nothing else, thorough and consistent, and the results that they produce are not arbitrary.  As such, we should seriously consider and work, when possible, to understand why they draw the conclusions they do, even when they seem nonsensical, as doing so often leads to new insights about the bias that we bring to our own research questions.\n"]},{"cell_type":"markdown","metadata":{"id":"04DbqCkQ8OLB"},"source":["# Interpretability"]},{"cell_type":"markdown","metadata":{"id":"XBcgatVuAWd3"},"source":["All machine learning process are of two primary types:   Interpretable and uninterpretable.  Interpretable models are a class of model that expose the processes behind and reasons why they produce the final results that they produce.  Interpretable models frequently do not perform as well as their non-interpretable counterparts; however, their interpretability offers significant value to humanities scholars, as the process of working to interpret process behind the results achieved is playing field of intellectual provocation and response through which we can advance our own understanding of the artifacts we study.\n","\n","When an interpretable model presents an unexpected result, we can investigate why, and learn something in the process.  When an uninterpretable process returns an unexpected result, we do not have acess to the computer's decision making process.  As such, it can be difficult to interrogate the final results.  \n","\n","In this module, we will focus on interpretable models—one each in each of the domains of comparison and classification.  This notebook provides ready to run code, so you will not be required to do any heavy lifting on the coding front.  Rather, you are encouraged to run each process multiple times on multiple sets of images in order to see how they perform.  In each case, as you work, try to determine why the computer is drawing the conclusion it is drawing, rather than simply assessing whether you believe the results are right or wrong.  When we convene as a group, we will discuss our insights while trying to understand how the computer thinks."]},{"cell_type":"markdown","metadata":{"id":"9JbO71nBAeVH"},"source":["# Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"J__iCigxAhff"},"source":["First we'll install all required packages and modules into the environment."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kIkne1tsPsqW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll need to get the data for today's session.  Because some of our previous work has written output files into our class data directory, we're going to completley remoce the diretory and re-clone the repository from GitHub."],"metadata":{"id":"NgKBklfMO85F"}},{"cell_type":"code","source":["# change directory into the course directory\n","%cd /content/drive/MyDrive/rbs_digital_approaches_2022/\n","\n","# remove the existing class data directory/repository contents\n","%rm -rf 2022_data_class\n","\n","# remove the directory itself\n","%rmdir -f 2022_data_class\n","\n","# now re-clone the repository from GitHub\n","!git clone https://github.com/RBS-Digital-Approaches/2022_data_class.git"],"metadata":{"id":"wtvHi7pIP_YP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we'll setup our Colab environment."],"metadata":{"id":"95MJH945WnwW"}},{"cell_type":"code","metadata":{"id":"i2snUk2s6VvE"},"source":["# install the scikit-learn package\n","!pip install scikit-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJRvJDEbSyH2"},"source":["# install the scikit-image package\n","!pip install scikit-image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbw5ZDk3ZhO-"},"source":["Now we import the packages we need into the environement."]},{"cell_type":"code","metadata":{"id":"Wb8zN-TNKvuG"},"source":["# import the openCV python package\n","# https://pypi.org/project/opencv-python/\n","# https://opencv.org/\n","import cv2\n","\n","# import the numpy package\n","# https://numpy.org/\n","import numpy as np\n","\n","# import scipy package and modules\n","# https://www.scipy.org/\n","import scipy\n","from scipy.cluster.hierarchy import dendrogram\n","\n","# import the networkx package\n","# https://networkx.org/\n","import networkx as nx\n","\n","# import the matplotlib package and modules\n","# https://matplotlib.org/\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# import the pystest package\n","# https://docs.pytest.org/en/6.2.x/\n","import pytest\n","\n","# import the random package\n","# https://docs.python.org/3/library/random.html\n","import random\n","\n","# import the scikit-learn package and module\n","# https://scikit-learn.org/stable/\n","import sklearn\n","from sklearn.cluster import AgglomerativeClustering\n","\n","# import the os library\n","# https://docs.python.org/3/library/os.html\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THRNKe1hZocc"},"source":["Here we define an input file path in the s2_class_data repo which contains several hundred images of woodcut impressions from the English Broadside Ballad Archive collection as well as an output directory where we will save files.  These directories will be reference by both sections of the notebook."]},{"cell_type":"code","metadata":{"id":"tdudVAMrQIsR"},"source":["in_path = \"/content/drive/MyDrive/rbs_digital_approaches_2022/2022_data_class/ebba_woodblocks/\"\n","out_path = \"/content/drive/MyDrive/rbs_digital_approaches_2022/output/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KEvkR7ToeaPk"},"source":["# Feature Identification and Extraction"]},{"cell_type":"markdown","metadata":{"id":"CO4RO_MiejDf"},"source":["In this section of the notebook we will explore basic image matching techniques.  Contrary to popular expectation, image mapping is rarely accomplished by comparing the shape(s) of object.  Shape matching (contour matching, to image specialists) is an extremely faulty process because slight changes in orientation (such as those produced by changes in the orientation, height, or lens type of the camera taking the image have impacts on the shapes of things as they appear in images, as we’ve already discussed in class. \n","\n","One method of image comparison that can work quite well with modern, color photography is color histogram analysis.  With this process, each pixel in an image is examined and assigned a numeric score based on the color present in the pixel.  This creates a matrix representation of the image of the same dimensions as the pixel dimensions of the image.  This matrix can be compared with other matrices for similarity as a means of finding a matching image.  \n","\n","As noted, this approach works well on color images with high contrast and a lot of objects pictured in them.  But it does not work well on black and white images, which comprise a great deal of the body of digitally available historical materials.  They also fail on high resolution color images of historical printed materials because both the structure and color of the paper on which they were printed changes from copy to copy, which changes the color score of the pixels.\n","\n","The preferred method of performing interpretable, computational image mapping is known as Bag of Features matcing. We’ll explain the approach *in situ* as we work through its application below.\n"]},{"cell_type":"markdown","metadata":{"id":"uiVt1_7dl-n8"},"source":["First, we'll load two images to compare from the previously defined working diectory.  Please take a moment to use the Google Drive web interface to naviagate to the './data_class/ebba_woodblocks/ dirctory in yor google drive  amd look at the photos we'll be using."]},{"cell_type":"code","metadata":{"id":"EhA6gU39QDcF"},"source":["# Load the first Image\n","img_file_name = \"rox-2-564-565.jpg\"\n","img_file = in_path + img_file_name\n","img = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IY0-uYeLX8Im"},"source":["# Load the second Image\n","img2_file_name = \"rox-1-100-101.jpg\"\n","img_file = in_path + img2_file_name\n","img2 = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5yH6eXdwZc3"},"source":["Note that in the code above, not only did we load the images, but we also converted them to greyschale.  This is because the color information in the photo has no impact on feature identification but does require significantly more computational power to pross.  Cost without benefit is never good, so we simply remove the color information."]},{"cell_type":"markdown","metadata":{"id":"8EQm9rIbmr9t"},"source":["Now that we've identified and loaded two images for comparison, we need to declare an instance of the ORB_create feature extraction class from the Python OpenCV package.  \n","\n","The Bag of Features approach discards the idea that images depict discreet objects (a picture of a person and a car, for example) in favor of the idea that they are really just containers for collections of small, mathematically definable features, such as circles and angles.   \n","\n","To perform Bag of Features matching, for each image in a collection the computer first decomposes (blurs and the intensifies contrast) the image and then locates any lines (contours and edges) that appear in it.  It then looks for any changes in direction in line segments that create measurable and calculable angles.  These locations in the image are called \"keypoints.\"  It then records the location of each keypoint along with its mathematial defintion in matrix form.  The keypoint and its description, combined, are called a feature.  And the matrix that stores all the features found in an image is known as a Bag of Features.\n","\n","[OpenCV](https://opencv.org/) is a long-running, opensource image recognition platform, written in C++ that can be invoked through a Python wrapper.  And ORB is one of many feature extraction algorithms that can be run in OpenCV.  The most widely used and highest functioning feature extraction algorithms are the SURF and SIFT algorithms.  Unfortunately, because both are patent protected, they are not allowed to be used in Google Collaborator.  As such, we have defaulted to the opensource ORB algorithm.\n"]},{"cell_type":"code","metadata":{"id":"BtHu99loQ1s6"},"source":["# create a feature extraction object\n","# that uses open-source ORB features.\n","orb = cv2.ORB_create()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jx7ED8s1pegh"},"source":["When performing feature extraction, you must tell the algorithm how many \"levels\" of extraction to perform.  Levels can be thought of as passes, where after each single pass and before the next a blurring filter is applied to the image.  This has the effect of gradually eroding smaller features and expanding larger one.  Here, we set the value to 3.  You should play around with this number and run different extractions to see how it affects the resulting features and matching."]},{"cell_type":"code","metadata":{"id":"O40rsH_0pYUG"},"source":["# determine the number of levels\n","orb.setNLevels(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgK2CDVNqa_M"},"source":["New we call the \"detectAndCompute\" function to genrate keypoints and descriptors for both of our test images."]},{"cell_type":"code","metadata":{"id":"qHvG-o7fRNui"},"source":["keypoints, descriptors = orb.detectAndCompute(img, None)\n","keypoints2, descriptors2 = orb.detectAndCompute(img2, None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yA-i7A4Zrc1C"},"source":["In the code cells below, we draw the features found in the first of our test images onto the image itself and save it to the Google Drive in the “output” directory of our course home directory. After you run the cell, look at the image to get an idea of where the computer has located features. Then, try a few more images. And then try the same images with different tunings.\n","\n","When features are drawn on an image, the size of the circle represents the size of the feature.  And the line that extends from center to circumference shows the direction the feature angle is facing."]},{"cell_type":"code","metadata":{"id":"IY-gpqnwRaEL"},"source":["# create the new image with kehpoints drawn on it\n","img_w_featues = cv2.drawKeypoints(img, keypoints, None, None, flags=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFdRnDzTa3PD"},"source":["Now we'll look at the image to see the identified keypoints."]},{"cell_type":"code","metadata":{"id":"PojL_fgca-XX"},"source":["# Google Colab won't let us view images directly\n","# so we'll add them to a plot as a hack\n","plt.figure(figsize=(10,10))\n","plt.xticks([])\n","plt.yticks([])\n","plt.grid(False)\n","plt.imshow(img_w_featues, cmap=plt.cm.binary)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HDvuCw16bPZn"},"source":["You can also save the keypoint image to your Google Drive."]},{"cell_type":"code","metadata":{"id":"7HP_80CrRxnB"},"source":["# write the keypoint image to Google Drive\n","out_img_path = out_path + \"features_\" + img_file_name\n","cv2.imwrite(out_img_path, img_w_featues )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Oh-8Sb9zLUV"},"source":["# Direct Comparison Matching"]},{"cell_type":"markdown","metadata":{"id":"bsEh1hbctSZo"},"source":["Now that we've learned how to find, extract, and draw features on an image, it's time to do some image matching.  This is accomplished by finding the keypoints on any two image and then comparing the two sets of keypoints, the more keypoints that match, the more likely that the two images are the same.\n","\n","Here, we need to say a few words about \"sameness\".  What exactly do we mean by \"same.\"  Do we mean identical?  If so then two images would be the same if, and only if, they were exact copies of each other.  If one were cropped slightly smaller, or if it was of only half of the original sheet, it would not match with a perfect image of the whole sheet.  And what about skew? \n","\n","It turns out, there’s a fair amount of difference in things that as humans we generally consider to be the same.  The Bag of Features method solves many of the above problems because feature descriptions are both rotation and scale invariant, so neither the rotation nor the scale of images being examined affects the outcome.   \n","\n","The sammple images that are pre-identifided below are designed to illustrate this point.  They are of wooblock impressions made on two diferent broadside ballads, with different surrounding maerial.  Once you run the match with these images, you should experiment with others."]},{"cell_type":"markdown","metadata":{"id":"alfUDGg-wC7P"},"source":["The first step in the matching process is to create an instance of the needed matcher class.  "]},{"cell_type":"code","metadata":{"id":"3iweTuZRZLG1"},"source":["# create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wgFj-b7xIzA"},"source":["Now that we have a matcher ready, we send it the two sets of feature descriptions for our two images and let it look for matches."]},{"cell_type":"code","metadata":{"id":"_cXW4xzTZSK8"},"source":["# Match descriptors.\n","matches = bf.match(descriptors, descriptors2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2klE_RnfxZqj"},"source":["We'll now sort the matches based on their mathematic similarity.  This will allow us to look at the most significant one."]},{"cell_type":"code","metadata":{"id":"vjAg8uTzZcaS"},"source":["# Sort them in the order of their distance.\n","matches = sorted(matches, key = lambda x:x.distance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDoR_aU4xnE_"},"source":["Now, we use the drawMatches function to draw the matches found between our two images on a new image."]},{"cell_type":"code","metadata":{"id":"5j6g4r03ZhAK"},"source":["# Draw first 50 matches.\n","image_w_matches = cv2.drawMatches(img, keypoints, img2, keypoints2, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uf9z1J8Obfax"},"source":["And view the result."]},{"cell_type":"code","metadata":{"id":"ZpOit7I8ZOJb"},"source":["    plt.figure(figsize=(10,10))\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(image_w_matches, cmap=plt.cm.binary)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVDGXn0JgXVd"},"source":["When you look at the matched image, you'll see that the majority of features in image 2, the smaller image with only one woodcut imporession, match to features found in the larger image.  Because the computer has found a high number of matching keypoints, it would determine that these images do match.  (In other words, match can be defind as a situation when the majority of keypoints in any image can be matched to any other image.)"]},{"cell_type":"markdown","metadata":{"id":"bt55twT0x1JJ"},"source":["We can also save the match image to disk for future viewing."]},{"cell_type":"code","metadata":{"id":"eIanqQDwaC1h"},"source":["out_img_path2 = out_path + \"matches.jpg\"\n","cv2.imwrite(out_img_path2, image_w_matches )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCs9yK-iinCs"},"source":["# Matching One to Many"]},{"cell_type":"markdown","metadata":{"id":"xfoR_lzJzuCu"},"source":["In this section of the notebook we'll work on matching across a larger image library.  In the previous section, we used a brute-force matching technique that returned positive only if a direct match of features was found.  In this section, we’ll use Cosine Similarity to give us a similarity ranking for each image.  Cosine Similarity is a statiscal measure of how similar any two vectors or matrices are.  In our case, the bag of features for each image is represented as a matrix, so we can use Cosine Similiarity to get a measure of the similarity between bags of features."]},{"cell_type":"markdown","metadata":{"id":"98HWTFbE0llP"},"source":["To start, we'll need to get a directory listing/ filelist for our input directory.  "]},{"cell_type":"code","metadata":{"id":"PlpvlbFtitoY"},"source":["file_list = os.listdir(in_path) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Frw8yjdj669j"},"source":["Now, we'll run our feature extraction on all images in the input directory.  To do this, we create an ORB object, setup some emtpy lists to hold features and descripters, then loop through each image in the directory and exract its features.  Note that running this cell could take several minutes."]},{"cell_type":"code","metadata":{"id":"ezxXgVxM68-7"},"source":["# define a feature point extractor\n","orb = cv2.ORB_create()\n","orb.setNLevels(5)\n","\n","# create and empty list to put features into\n","features_list = []\n","features_dict = dict()\n","image_names = []\n","image_descriptors = []\n","# loop through files and process\n","for next_file in file_list:\n","  # define the file path\n","  next_path = in_path + next_file\n","  # define a size for the descriptors vector\n","  needed_size = 50000\n","\n","  # read in the file\n","  img = cv2.imread(next_path, cv2.IMREAD_GRAYSCALE)\n","  # calculate the keypoints\n","  #keypoints, descriptors = orb.detectAndCompute(img, None)\n","\n","  # compute kepoints\n","  kps = orb.detect(img)\n","\n","  # only keeping the largest 32 keypoints of each image\n","  kps = sorted(kps, key=lambda x: -x.response)[:32]\n","\n","  # compute the descriptors vector\n","  kps, dsc = orb.compute(img, kps)\n","  # Flatten all of them in one big vector - our feature vector\n","  dsc = dsc.flatten()\n","  # Making descriptor of same size as Descriptor vector size is 64\n","  if dsc.size < needed_size:\n","    # if we have less the 32 descriptors then just adding zeros at the\n","    # end of our feature vector\n","     dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n","\n","    # reshape the vector\n","    #dsc.reshape(1, -1)\n","\n","  # make a list that associates the filename with the keypoints\n","  temp_list = [next_file, dsc]\n","  # add the single image data to the list of lists\n","  features_list.append(temp_list)\n","  features_dict.update({next_file: dsc})\n","\n","  image_names.append(next_file)\n","  image_descriptors.append(dsc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7u498ZcMSi-W"},"source":["We'll designate the file we want to use as our seed here.  This image will be compared to all other images.  "]},{"cell_type":"code","metadata":{"id":"rrGGN4FuS9LR"},"source":["# first set an image you want to test from\n","# the set of processed images\n","test_image = \"20021-10.jpg\"\n","\n","# get the index for this image\n","test_index = image_names.index(test_image)\n","\n","# get the descriptors for this image\n","test_descriptor = image_descriptors[test_index]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTT9q7djOdKZ"},"source":["test_descriptor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvPRLh-T2pRR"},"source":["Now we loop through every other image in the dataset and calculate the similarity score between the two images.  Note the commented out \"np.dot\" line in the for loop.  Dot Multiplication is another statistical method for assessing the similarity between vectors. If you have time, switch between the two siilarity calculations and see what effect it has on the outcome."]},{"cell_type":"code","metadata":{"id":"1QenmHwnYMlE"},"source":["# create a list of comparision scores for this\n","# image against all images in set\n","sim_list = []\n","for i in range(len(image_descriptors)):\n","  #sim = np.dot(image_descriptors[test_index], image_descriptors[i])\n","  sim = scipy.spatial.distance.cosine(image_descriptors[test_index], image_descriptors[i])\n","  sim_list.append(sim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UgffC7b_3XaU"},"source":["Now that we've calculated all the similarity scores, we'll remove the original seed image from the dataset, otherwise we'd be comparing it to itself."]},{"cell_type":"code","metadata":{"id":"CdZCa2sCSory"},"source":["# remove test image from the data\n","img_names_temp = image_names\n","sim_list_temp = sim_list\n","del img_names_temp[test_index]\n","del sim_list_temp[test_index]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ywOcmz63hsv"},"source":["We also have to do some data munging.  We need to sort our results so that when we plot them they will be more readable.  The best way to do this is to take our list of image names and our list of similarity scores and join them as a dictionay, that way we can sort without loosing name/score associations."]},{"cell_type":"code","metadata":{"id":"Yv8V8dEsgHLk"},"source":["# join our two lists as a dictionary\n","sim_dict = {img_names_temp[i]: sim_list_temp[i] for i in range(len(sim_list_temp))}\n","# now sort the dictionary\n","sim_dict = dict(sorted(sim_dict.items(), key=lambda x: x[1], reverse=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSXTIrt2wO-1"},"source":["First, we'll plot the similarity scores for the entire dataset."]},{"cell_type":"code","metadata":{"id":"CIyE5QPewVkt"},"source":["import matplotlib.pyplot as plt\n","# set the size of the plot\n","fig = plt.figure(figsize=(12, 9))\n","ax = fig.add_axes([0,0,1,1])\n","ax.bar(sim_dict.keys(), sim_dict.values())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iJGSEFH4YaQ"},"source":["The above visualization is useful for giving us an overall idea of shape of our data, but there are too many datapoints for the graph to be truly legible.  To solve this, let's subset the results and only look at the top most related images."]},{"cell_type":"code","metadata":{"id":"j0Bui-u2sw84"},"source":["# subset first 20 items of list for plotting\n","sim_dict_subset = dict(list(sim_dict.items())[0: 10]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSzqqWugceKQ"},"source":["# draw the plot\n","import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(12, 9))\n","ax = fig.add_axes([0,0,1,1])\n","ax.bar(sim_dict_subset.keys(), sim_dict_subset.values())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View the results"],"metadata":{"id":"Y5ubwjJBf32m"}},{"cell_type":"code","source":["# first draw the test image\n","img_m = in_path + test_image\n","this_img = cv2.imread(img_m)\n","plt.figure(figsize=(5,5))\n","plt.xticks([])\n","plt.yticks([])\n","plt.grid(False)\n","plt.imshow(this_img, cmap=plt.cm.binary)\n","plt.show()\n","\n","# now draw the matches\n","for key in sim_dict_subset:\n","  img_m_1 = in_path + key\n","  this_img = cv2.imread(img_m_1)\n","  plt.figure(figsize=(3,3))\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.grid(False)\n","  plt.imshow(this_img, cmap=plt.cm.binary)\n","  plt.show()"],"metadata":{"id":"s9uyI3fEexBl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jBn81D9r9aa"},"source":["# Clustering on Feature Similarity"]},{"cell_type":"markdown","metadata":{"id":"zYmO6B_tsC6H"},"source":["In the above section, we saw how assess the similarity of a single image to a library of images.  In this section we'll examine clustering as an approach to examining the similarity of all images in a collection to each other based on their features.\n","\n","In statistics, Cluster analysis, or clustering, is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n","\n","We've already built a collection of feature descriptors for each image in our collection.  To cluster these, we'll first model the similarity between the feature descriptors in our colleciton and then plot then plot a similarity network."]},{"cell_type":"markdown","metadata":{"id":"YR91K-TH1O7E"},"source":["First, let's build a function for plotting a dendogram.  After we build our cluster model, we'll send the model to this function to create the actual dendogram."]},{"cell_type":"code","metadata":{"id":"PFK_-p1omONr"},"source":["# define a function that receives an hierarchical\n","# cluster model and a collection of keywords as arguments\n","def plot_dendrogram(model, **kwargs):\n","\n","    # first, count the number of samples \n","    # that appear under each node of the hierarchy\n","    counts = np.zeros(model.children_.shape[0])\n","    n_samples = len(model.labels_)\n","    for i, merge in enumerate(model.children_):\n","        current_count = 0\n","        for child_idx in merge:\n","            if child_idx < n_samples:\n","                current_count += 1  # leaf node\n","            else:\n","                current_count += counts[child_idx - n_samples]\n","        counts[i] = current_count\n","\n","    # now that we have the node counts, we can create a \"linkage\"\n","    # matrix that stores the node relationships for each image\n","    linkage_matrix = np.column_stack([model.children_, model.distances_,\n","                                      counts]).astype(float)\n","\n","    # now we'll plot the linkage_matrix\n","    dendrogram(linkage_matrix, **kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQP1A1zT2mVs"},"source":["Now we'll create an hierarchical model object and then fit the model to our image descriptors"]},{"cell_type":"code","metadata":{"id":"0AxUTbpBprrM"},"source":["# create the model. \n","# setting distance_threshold=0 ensures we compute the full tree.\n","model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n","# fit the model\n","model = model.fit(image_descriptors)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RCPqiXNr2-nJ"},"source":["Now lets plot the entire similarity hierarchy"]},{"cell_type":"code","metadata":{"id":"qSENp5bnp2NV"},"source":["plt.figure(figsize=(12, 9))\n","plt.title('Hierarchical Clustering Dendrogram: All Images')\n","plot_dendrogram(model)\n","plt.xlabel(\"Index of Image (in 'image_names' list)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMwWv62c3Foc"},"source":["As you can see, there are too many images in our model to visualize any discrete relationships between individual images.  The complete dendogram gives us a good sense of how similarity plays out across the entire collection, but you can't really tell which images are related to which other images.  \n","\n","To remedy this, we'll next produce a truncated plot where we only plot details for three levels of the hierarchy."]},{"cell_type":"code","metadata":{"id":"WauMfHzyqFA2"},"source":["plt.figure(figsize=(12, 9))\n","plt.title('3 Level Hierarchical Clustering Dendrogram')\n","plot_dendrogram(model, truncate_mode='level', p=3)\n","plt.xlabel(\"Number of points in node (or index of point if no parenthesis)\")\n","plt.show()"],"execution_count":null,"outputs":[]}]}