{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_d5_m2_image_classification.ipynb","provenance":[{"file_id":"1HfIoDA6NR8IC3MDFriurhap_QBLmHkyG","timestamp":1626415739040}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"toWcaQT8a42x"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"vwU8gqhY9Wm1"},"source":["This module focuses on using a Convolutional Neural Network (CNN), or Deep Learning Models as they are sometimes called, to classify images.  CNN’s are quite good at performing classification tasks; however, they require a significant amount of labeled training data and are also uninterpretable.  \n","\n","During this module we will build our neural network on a publicly available (but not very interesting) dataset.  This is because the dataset is pre-packaged and ready to use in the Google Collab environment.  This avoids any of the many problems that frequently arise when we need to move large amounts of data around in the cloud.\n","\n","There are three primary phases involved with using neural networks for classification.  First, you must load two sets of data:  a labeled training set and a test set.  The labeled training set consists of several thousand identified images of each class into which you ultimately want to classify your images.  The test set consists of all the images for which you want to know the class. \n","\n","Next, you must setup and configure the neural network infrastructure.  Thanks to publicly available packages like Tenserflow, this can now be done with only a few lines of code. \n","\n","Once you have loaded your data, you must train the network.  This involves feeding into the neural network a collection of labelled training data.  For example, if I were trying to train a neural network to distinguish between cats and dogs, I would give it several thousand example cat pictures and then several thousand example dog pictures.  The CNN would then “learn” the difference between cats and dogs from these pictures.\n","\n","Finally, once we have trained our CNN, we can than give it novel images and ask it to classify them.  As stated earlier, CNN’s are quite good at this task.  However, they provide us no information about how they arrive at their conclusions; so there is no way to interrogate them the way we do other statistical models.  This is why people often refer to neural networks as a “black box,” and it is also why they are only appropriate as digital humanities tool when we have no desire to do such an interrogation and care only about the classification outcome. \n"]},{"cell_type":"markdown","metadata":{"id":"h12wJeT1DMCk"},"source":["# Setup Python Environment"]},{"cell_type":"markdown","metadata":{"id":"Kw9jx42hDh6D"},"source":["Befire we can begin setting up our CNN, we must import necessarh packages and modules"]},{"cell_type":"code","metadata":{"id":"wKlmmldOZO8V"},"source":["# import TensorFlow package\n","# https://www.tensorflow.org/\n","import tensorflow as tf\n","\n","# import numpy\n","# https://numpy.org/\n","import numpy as np\n","\n","# import matplotlib\n","# https://matplotlib.org/\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57XoYlTuZrmR"},"source":["# Load the MNIST Fashion Dataset"]},{"cell_type":"markdown","metadata":{"id":"G7y8uMDpEhWe"},"source":["Now that we have our enironment ready, we can start loding and preprocessign our data.  For this exercise, we will work with the opensource [MNIST Fashion Dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist), becase it is already available on Google Collab. Loading the dataset returns four NumPy arrays:\n","\n","*   The train_images and train_labels arrays are the training set—the data the model uses to learn.\n","*   The model is tested against the test set, the test_images, and test_labels arrays."]},{"cell_type":"code","metadata":{"id":"Jz-2RR5MZh5D"},"source":["# instantiate a mnist object\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","# get image sets and their labels\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEHrsSl7wmxl"},"source":["train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NLtGx1-Wah6x"},"source":["Each image is mapped to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:"]},{"cell_type":"code","metadata":{"id":"hH44Kem-aktY"},"source":["class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMpN4_G4ayOx"},"source":["# Preprocess Data"]},{"cell_type":"markdown","metadata":{"id":"hziHW3HHbWjW"},"source":["The data must be preprocessed before training the network. If you inspect the first image in the training set by running the code below, you will see that the pixel colorvalues fall in the range of 0 to 255:"]},{"cell_type":"code","metadata":{"id":"F2OsPJpxa-nj"},"source":["plt.figure()\n","plt.imshow(train_images[0])\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8J3xksyDbTOS"},"source":["This is more data than we need to successfully classify, so we'll scale them to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the training set and the testing set be preprocessed in the same way:"]},{"cell_type":"code","metadata":{"id":"NtUQ4lFWbkVP"},"source":["# scale training data\n","train_images = train_images / 255.0\n","# scale testing data\n","test_images = test_images / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QxrgyPONeVky"},"source":["Here's what the same image looks like after color value reduction. "]},{"cell_type":"code","metadata":{"id":"6fJuLUpPeBfo"},"source":["plt.figure()\n","plt.imshow(train_images[0])\n","plt.colorbar()\n","plt.grid(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3AqSF_pfc-T"},"source":["As you can see, the color value rendering of the image hasn't changes much to the naked eye.  But it is now computationally smaller and will process faster.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IKF6TiDpbrR-"},"source":["Take a look at a larger sample of the images to make sure they still look good."]},{"cell_type":"code","metadata":{"id":"ialfDputbvvS"},"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_images[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[train_labels[i]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8mgE6Atb4ju"},"source":["# Configure The Model"]},{"cell_type":"markdown","metadata":{"id":"5au4fJhccAim"},"source":["The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are learned during training."]},{"cell_type":"code","metadata":{"id":"Nii9XpqkcIRO"},"source":["# instantiate the model with parameters\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dense(10)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RocPYzzscOHc"},"source":["The first layer in this network, tf.keras.layers.Flatten, transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n","\n","After the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely connected, or fully connected, neural layers. The first Dense layer has 128 nodes (or neurons). The second (and last) layer return an array with length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes.\n","\n","Once we've defined the model, we have to 'compile' it to prepare it for use.  This is the step that truly bring the model into being.  The parameters of our compilation are as follows:"]},{"cell_type":"markdown","metadata":{"id":"gGCX9ZbJcZd9"},"source":["\n","    Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n","    Optimizer —This is how the model is updated based on the data it sees and its loss function.\n","    Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n"]},{"cell_type":"code","metadata":{"id":"QgqEe0UAcxmA"},"source":["# compile the model for use\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhiLvYZDc2SN"},"source":["# Train the Model"]},{"cell_type":"markdown","metadata":{"id":"355E2Meqc6lx"},"source":["Training the neural network model requires the following steps:\n","\n","    Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n","    The model learns to associate images and labels.\n","    You ask the model to make predictions about a test set—in this example, the test_images array.\n","    Verify that the predictions match the labels from the test_labels array.\n"]},{"cell_type":"code","metadata":{"id":"bowA5QGtdP0l"},"source":["# fit the model to the training set (fit goes\n","# both ways for trained models.  First we fit\n","# the model to ground truth, then we fit our\n","# test object to the model.). The 'epochs' param\n","# establishes how many times the training should\n","# cycle images through the network\n","model.fit(train_images, train_labels, epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4DIRBuuOeHjP"},"source":["# Make Predictions"]},{"cell_type":"markdown","metadata":{"id":"HB-ZmyBzeL5I"},"source":["With the model trained, you can use it to make predictions about some images. The modela complex matrix of values that are diffiult to interpret. In order to make it easier to understand the prediction, we attach a final \"softmax\" (the name of the redudtion algorithm) layer to convert the matrix values to probabilities, which are easier to interpret. "]},{"cell_type":"code","metadata":{"id":"QGu9_IwUeSyh"},"source":["probability_model = tf.keras.Sequential([model, \n","                                         tf.keras.layers.Softmax()])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzmcGS0kebd1"},"source":["Now we input our test images to get predictions."]},{"cell_type":"code","metadata":{"id":"ZXl-1BK4eagK"},"source":["# run the model with test images.\n","predictions = probability_model.predict(test_images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeHQeQ3UekM2"},"source":["A prediction is an array of 10 numbers.  There is one element in the array for each of the class that we trained the model to recognize.  Each of the ten numbers in the prediction array indicate the model's \"confidence\" that the image corresponds to each of the 10 different articles of clothing."]},{"cell_type":"markdown","metadata":{"id":"MKXFsELZkUV1"},"source":["First we'll look at the raw data in the predictions matrix for a single item."]},{"cell_type":"code","metadata":{"id":"uofaLF70epZE"},"source":["# print the prediction for the first test image\n","# to see the prediction result for that image.\n","predictions[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_gknznMeyll"},"source":["You can see which label has the highest confidence value with the following command:"]},{"cell_type":"code","metadata":{"id":"AxnmtWTMezKL"},"source":["np.argmax(predictions[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHNaen8le98S"},"source":["Here, we'll define a couple of functions to show us the results in a more human readable form.  See if you can follow what the code is doing.  (Reading existing code is a good way to advance your own coding skill.)"]},{"cell_type":"code","metadata":{"id":"knKbz6oxfDUS"},"source":["def plot_image(i, predictions_array, true_label, img):\n","  true_label, img = true_label[i], img[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","\n","  plt.imshow(img, cmap=plt.cm.binary)\n","\n","  predicted_label = np.argmax(predictions_array)\n","  if predicted_label == true_label:\n","    color = 'blue'\n","  else:\n","    color = 'red'\n","\n","  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n","                                100*np.max(predictions_array),\n","                                class_names[true_label]),\n","                                color=color)\n","\n","def plot_value_array(i, predictions_array, true_label):\n","  true_label = true_label[i]\n","  plt.grid(False)\n","  plt.xticks(range(10))\n","  plt.yticks([])\n","  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n","  plt.ylim([0, 1])\n","  predicted_label = np.argmax(predictions_array)\n","\n","  thisplot[predicted_label].set_color('red')\n","  thisplot[true_label].set_color('blue')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CWJWLCAfj33"},"source":["First, let's call our function on a single test image to see what the results look like.  You can look at other predictions by changing the value of the indix variable, \"i\", and running the cell again."]},{"cell_type":"code","metadata":{"id":"H7qiMFbBfnpC"},"source":["# plot the prediction for a single test image\n","i = 0\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions[i], test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions[i],  test_labels)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULCJJmlDfy9y"},"source":["Let's now look at results for several of the test imgages."]},{"cell_type":"code","metadata":{"id":"9dcCidZTf47P"},"source":["# Plot the first X test images, their predicted labels, and the true labels.\n","# Color correct predictions in blue and incorrect predictions in red.\n","num_rows = 5\n","num_cols = 3\n","num_images = num_rows*num_cols\n","plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n","for i in range(num_images):\n","  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n","  plot_image(i, predictions[i], test_labels, test_images)\n","  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n","  plot_value_array(i, predictions[i], test_labels)\n","plt.tight_layout()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwOie_Z0hw6k"},"source":["Single Image Prediction"]},{"cell_type":"code","metadata":{"id":"Wvx3cvp-iD2f"},"source":["# Grab an image from the test dataset.\n","img = test_images[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uu2_MQ4OiPed"},"source":["tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:"]},{"cell_type":"code","metadata":{"id":"eq-t0bbliQgM"},"source":["# Add the image to a batch where it's the only member.\n","img = (np.expand_dims(img,0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDTuwiZziYIa"},"source":["Now predict the label for the image"]},{"cell_type":"code","metadata":{"id":"hOVBxDPMiXeI"},"source":["predictions_single = probability_model.predict(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yO--rUJtiiGd"},"source":["Look at the prediction Results"]},{"cell_type":"code","metadata":{"id":"cQ5Cn-Y2iocW"},"source":["print(predictions_single)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UaZdeMgIiukM"},"source":["Plot the Result"]},{"cell_type":"code","metadata":{"id":"T_2D70XBiw28"},"source":["plot_value_array(1, predictions_single[0], test_labels)\n","_ = plt.xticks(range(10), class_names, rotation=45)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tkQJLpd7mI2k"},"source":["<font size=\"1\">\n","This workbook is based heavily on the TensorFlow ML basics with Keras tutorial on [Basic image classification](https://www.tensorflow.org/tutorials/keras/classification) and is similarly made available under the MIT license:\n","\n","\n","> <font size=\"1\">\\# MIT License\n","> \\#\n","> \\# Copyright (c) 2017 François Chollet\n","> \\#\n","> \\# Permission is hereby granted, free of charge, to any person obtaining a\n","> \\# copy of this software and associated documentation files (the \"Software\"),\n","> \\# to deal in the Software without restriction, including without limitation\n","> \\# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n","> \\# and/or sell copies of the Software, and to permit persons to whom the\n","> \\# Software is furnished to do so, subject to the following conditions:\n","> \\#\n","> \\# The above copyright notice and this permission notice shall be included in\n","> \\# all copies or substantial portions of the Software.\n","> \\#\n","> \\# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","> \\# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","> \\# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n","> \\# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","> \\# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n","> \\# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n","> \\# DEALINGS IN THE SOFTWARE.\n","\n","\n"]}]}